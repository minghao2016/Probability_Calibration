\documentclass[ignorenonframetext,]{beamer}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\else % if luatex or xelatex
\ifxetex
\usepackage{mathspec}
\else
\usepackage{fontspec}
\fi
\defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
\usetheme{Dresden}
\usecolortheme{seahorse}
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\newif\ifbibliography

% Prevent slide breaks in the middle of a paragraph:
\widowpenalties 1 10000
\raggedbottom

\AtBeginPart{
\let\insertpartnumber\relax
\let\partname\relax
\frame{\partpage}
}
\AtBeginSection{
\ifbibliography
\else
\let\insertsectionnumber\relax
\let\sectionname\relax
\frame{\sectionpage}
\fi
}
\AtBeginSubsection{
\let\insertsubsectionnumber\relax
\let\subsectionname\relax
\frame{\subsectionpage}
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
\usepackage{tikz}
\usepackage{pgfplots}

\title{Calibration for probabilistic classification}
\subtitle{Nick}
\date{}

\begin{document}
\frame{\titlepage}

\section{Overview}\label{overview}

\begin{frame}{The problem}

\begin{itemize}
\tightlist
\item
  some classifiers produce well-calibrated probabilities

  \begin{itemize}
  \tightlist
  \item
    \textbf{examples:} discriminant analysis, logistic regression
    \vspace{2mm}
  \end{itemize}
\item
  others don't

  \begin{itemize}
  \tightlist
  \item
    \textbf{examples:} naive bayes, SVMs, anything with boosting,
    tree-based methods, sometimes neural networks
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{First of all, who cares?}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  people with asymmetric misclassification costs
\item
  people who are going to use the scores in post-processing
\item
  people who want to compare model outputs on a fair basis
\end{enumerate}

\end{frame}

\begin{frame}{Definitions: ``classification''}

in general, a classifier is a mapping function \(f\) such that

\[f: \vec{x} \mapsto c\]

where \(\vec{x} \in \mathbb{R}^{P}\), but we're mostly interested in the
intermediate step in where the function produces some membership score
\(s_i\) for each instance \(\vec{x}_i\)

\end{frame}

\begin{frame}{Definitions: ``well-calibrated''}

\begin{itemize}
\tightlist
\item
  for a model \(f\) and score \(s_i\) to be well-calibrated for class
  \(c_i\), the empirical probability of a correct classification
  \(P(c_i | f( c_i | x_i)=s_i)\) must converge to \(f(c_i | x_i) = s_i\)
  \vspace{5mm}
\item
  \textbf{example}: When \(s_i = 0.9\), the probability of a correct
  classification should converge to \(P(c_i | s_i = 0.9) = 0.9\).
  Otherwise, this isn't \textit{really} a `probability.'
\end{itemize}

\end{frame}

\begin{frame}{Definitions: ``calibration''}

the calibration process is a separate mapping such that

\[g: s_i \mapsto P(c_i | s_i)\]

\textbf{it's really important to note that we're fitting another model
on top of our model output, where your feature matrix is just the vector
of probability scores \(\vec{s}\) and the target variable is the vector
of true class labels \(\vec{y} \in {0,1}\)}

\end{frame}

\section{Motivation}\label{motivation}

\section{Common methods}\label{common-methods}

\section{\texorpdfstring{Extensions to
\(k > 2\)}{Extensions to k \textgreater{} 2}}\label{extensions-to-k-2}

\begin{frame}{Probabilistic classification as a simplex}

\begin{itemize}
\item
  we view the task of probabilistic classification as a vector-valued
  function, we can visualize the co-domain of this task as assigning the
  location of a prediction in a regular (unit) simplex, \(\Delta^{K-1}\)
\item
  why is this hard when \(K > 2\)?
\end{itemize}

\end{frame}

\begin{frame}{Probabilistic classification as a simplex}

\begin{figure}
\begin{tikzpicture}
\node [above] at (0, 1.5) {$\Delta^{1}$};
\node [above] at (2.5, 1.5) {$\Delta^{2}$};
%\node [below] at (0, -.25) {line segment};
%\node [below] at (2.5, -.25) {triangle};

\draw [black, thick] (0, 0) -- (0, 1);
\draw [black, fill =black] (0, 0) circle [radius = 0.1];
\draw [black, fill =black] (0, 1) circle [radius = 0.1];
\draw [black, thick] (2, 0) -- (3, 0) -- (2.5, 0.87) -- (2,0);
\draw [black, fill = black](2, 0) circle [radius = 0.1];
\draw [black, fill = black](3, 0) circle [radius = 0.1];
\draw [black, fill = black](2.5, 0.87) circle [radius = 0.1];

\end{tikzpicture}
\end{figure}

trivial with \(\Delta^{1}\) because we're only concerned with one
unknown value and its complement. With \(\Delta^{K>2}\) the simplex
becomes a triangle, tetrahedron, five-cell, etc.

\end{frame}

\begin{frame}{Multi-class probability estimation}

\begin{figure}
\begin{tikzpicture}
\draw [black] (-.5,-.5) rectangle (8, .5);
\draw [blue, fill =purple] (5.5,0) circle [radius = 0.2];
\draw [blue, fill =blue] (0,0) circle [radius = 0.2];
\draw [blue, fill =blue] (.5,0) circle [radius = 0.2];
\draw [blue, fill =blue] (1,0) circle [radius = 0.2];
\draw [blue, fill =red] (1.5,0) circle [radius = 0.2];
\draw [blue, fill =red] (2,0) circle [radius = 0.2];
\draw [blue, fill =red] (2.5,0) circle [radius = 0.2];
\draw [blue, fill =red] (3,0) circle [radius = 0.2];
\draw [blue, fill =orange] (3.5,0) circle [radius = 0.2];
\draw [blue, fill =orange] (4,0) circle [radius = 0.2];
\draw [blue, fill =purple] (4.5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (5.5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (6,0) circle [radius = 0.2];
\draw [blue, fill =purple] (6.5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (7,0) circle [radius = 0.2];
\draw [blue, fill =purple] (7.5,0) circle [radius = 0.2];
\end{tikzpicture}
\caption {classification problem with $k = 4$}
\end{figure}

\begin{block}{\textbf{Strategy:} decompose into separate binary
classification problems}

\begin{itemize}
\tightlist
\item
  one vs.~all
\item
  all pairs
\end{itemize}

\end{block}

\end{frame}

\begin{frame}{One vs.~all}

\begin{figure}
\begin{tikzpicture}
\draw [black] (-.5,-.5) rectangle (8, .5);
\node [above] at (0.5, .75) {\emph{one}};
\draw [->, thick] (0.5, .75) -- (0.5, .3);

\node [above] at (4.25, .75) {\emph{all}};
\draw [->, thick] (4.25, .75) -- (2.25, .3);
\draw [->, thick] (4.25, .75) -- (3.75, .3);
\draw [->, thick] (4.25, .75) -- (6, .3);


\draw [red, ultra thick] (-.25, -.25) rectangle (1.22, .25);
\draw [red, ultra thick] (1.27, -.25) rectangle (7.75, .25);
\draw [blue, fill =blue] (0,0) circle [radius = 0.2];
\draw [blue, fill =blue] (.5,0) circle [radius = 0.2];
\draw [blue, fill =blue] (1,0) circle [radius = 0.2];
\draw [blue, fill =red] (1.5,0) circle [radius = 0.2];
\draw [blue, fill =red] (2,0) circle [radius = 0.2];
\draw [blue, fill =red] (2.5,0) circle [radius = 0.2];
\draw [blue, fill =red] (3,0) circle [radius = 0.2];
\draw [blue, fill =orange] (3.5,0) circle [radius = 0.2];
\draw [blue, fill =orange] (4,0) circle [radius = 0.2];
\draw [blue, fill =purple] (4.5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (5.5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (6,0) circle [radius = 0.2];
\draw [blue, fill =purple] (6.5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (7,0) circle [radius = 0.2];
\draw [blue, fill =purple] (7.5,0) circle [radius = 0.2];
\end{tikzpicture}
\caption {\emph{one vs. all} reduces to $k-1$ calibrations}
\end{figure}

\end{frame}

\begin{frame}{All pairs}

\begin{figure}
\begin{tikzpicture}
\draw [black] (-.5,-.5) rectangle (8, .5);
\node [above] at (0.5, .75) {$k_i$};
\draw [->, thick] (0.5, .75) -- (0.5, .3);

\node [above] at (2.25, .75) {$k_j$};
\draw [->, thick] (2.25, .75) -- (2.25, .3);

\draw [red, ultra thick] (-.25, -.25) rectangle (1.22, .25);
\draw [red, ultra thick] (1.27, -.25) rectangle (3.25, .25);
\draw [blue, fill =blue] (0,0) circle [radius = 0.2];
\draw [blue, fill =blue] (.5,0) circle [radius = 0.2];
\draw [blue, fill =blue] (1,0) circle [radius = 0.2];
\draw [blue, fill =red] (1.5,0) circle [radius = 0.2];
\draw [blue, fill =red] (2,0) circle [radius = 0.2];
\draw [blue, fill =red] (2.5,0) circle [radius = 0.2];
\draw [blue, fill =red] (3,0) circle [radius = 0.2];
\draw [blue, fill =orange] (3.5,0) circle [radius = 0.2];
\draw [blue, fill =orange] (4,0) circle [radius = 0.2];
\draw [blue, fill =purple] (4.5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (5.5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (6,0) circle [radius = 0.2];
\draw [blue, fill =purple] (6.5,0) circle [radius = 0.2];
\draw [blue, fill =purple] (7,0) circle [radius = 0.2];
\draw [blue, fill =purple] (7.5,0) circle [radius = 0.2];
\end{tikzpicture}
\caption {\emph{all pairs} reduces to ${K}\choose{2}$ calibrations}
\end{figure}

\end{frame}

\begin{frame}{Combining multi-class probability estimates}

\end{frame}

\section{Experimental results}\label{experimental-results}

\section{Conclusion}\label{conclusion}

\end{document}
